{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GeForce GTX 1080 Ti', 1, True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.get_device_name(0), torch.cuda.device_count(), torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GeForce GTX 1080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary \n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, X, transform):\n",
    "        self.X = X\n",
    "        self.transform = transform \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.X[index]\n",
    "        # sample = torch.tensor(r)\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample, sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndef min_max_normalization(tensor, min_value, max_value):\\n    min_tensor = tensor.min()\\n    tensor = (tensor - min_tensor)\\n    max_tensor = tensor.max()\\n    tensor = tensor / max_tensor\\n    tensor = tensor * (max_value - min_value) + min_value\\n    return tensor\\n\\n\\ndef tensor_round(tensor):\\n    return torch.round(tensor)\\n\\nimg_transform = transforms.Compose([\\n    transforms.ToTensor(),\\n    transforms.Lambda(lambda tensor:min_max_normalization(tensor, 0, 1)),\\n    transforms.Lambda(lambda tensor:tensor_round(tensor))\\n])\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.load('x_train.npy')\n",
    "x_cv = np.load('x_cv.npy') \n",
    "\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # parameters mean, std are passed as 0.5, 0.5 in your case for each channe;\n",
    "    # This will normalize the image in the range [-1,1]\n",
    "    # image = (image - mean) / std \n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def min_max_normalization(tensor, min_value, max_value):\n",
    "    min_tensor = tensor.min()\n",
    "    tensor = (tensor - min_tensor)\n",
    "    max_tensor = tensor.max()\n",
    "    tensor = tensor / max_tensor\n",
    "    tensor = tensor * (max_value - min_value) + min_value\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def tensor_round(tensor):\n",
    "    return torch.round(tensor)\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda tensor:min_max_normalization(tensor, 0, 1)),\n",
    "    transforms.Lambda(lambda tensor:tensor_round(tensor))\n",
    "])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_processed_train = Data(x_train, img_transform)\n",
    "x_processed_cv = Data(x_cv, img_transform)\n",
    "\n",
    "X_train_data = DataLoader(x_processed_train, batch_size=batch_size, shuffle=True)\n",
    "X_cv_data = DataLoader(x_processed_cv, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for data in X_train_data:\n",
    "    # torch.Size([248, 64, 64, 3])\n",
    "    print(data.shape)\n",
    "\"\"\"\n",
    "\n",
    "def to_img(x):\n",
    "    x = x.view(x.size(0), 3, 64, 64)\n",
    "    return x\n",
    "\n",
    "def plot_sample_img(img, name):\n",
    "    img = img.view(3, 64, 64)\n",
    "    save_image(img, 'test/sample_{}.png'.format(name))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(\"EarlyStopping counter: {} out of {}\".format(self.counter, self.patience))\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(\"Validation loss decreased ({:.6f} --> {:.6f}). Saving model ...\".format(self.val_loss_min, val_loss))\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vanilla_autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(vanilla_autoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(nn.BatchNorm1d(num_features=3 * 64 * 64,\n",
    "                                                    track_running_stats=False),\n",
    "                                     nn.Linear(in_features=3 * 64 * 64,\n",
    "                                               out_features=4000,\n",
    "                                               bias=True),\n",
    "                                     nn.ELU(alpha=1.0,\n",
    "                                            inplace=True),\n",
    "                                     nn.Dropout(p=0.5,\n",
    "                                                inplace=False),\n",
    "                                     nn.BatchNorm1d(num_features=4000,\n",
    "                                                    track_running_stats=False),\n",
    "                                     nn.Linear(in_features=4000,\n",
    "                                               out_features=1000,\n",
    "                                               bias=True),\n",
    "                                     nn.ELU(alpha=1.0,\n",
    "                                            inplace=True),\n",
    "                                     nn.Dropout(p=0.5,\n",
    "                                                inplace=False),\n",
    "                                     nn.BatchNorm1d(num_features=1000,\n",
    "                                                    track_running_stats=False),\n",
    "                                     nn.Linear(in_features=1000,\n",
    "                                               out_features=300,\n",
    "                                               bias=True),\n",
    "                                     nn.ELU(alpha=1.0,\n",
    "                                            inplace=True), \n",
    "                                     nn.BatchNorm1d(num_features=300,\n",
    "                                                    track_running_stats=False),\n",
    "                                     nn.Linear(in_features=300,\n",
    "                                               out_features=150,\n",
    "                                               bias=True),\n",
    "                                     nn.Sigmoid())\n",
    "        \n",
    "        self.decoder = nn.Sequential(nn.BatchNorm1d(num_features=150,\n",
    "                                                    track_running_stats=False),\n",
    "                                     nn.Linear(in_features=150,\n",
    "                                               out_features=300,\n",
    "                                               bias=True),\n",
    "                                     nn.ELU(alpha=1.0,\n",
    "                                            inplace=True),\n",
    "                                     nn.Dropout(p=0.5,\n",
    "                                                inplace=False),\n",
    "                                     nn.BatchNorm1d(num_features=300,\n",
    "                                                    track_running_stats=False),\n",
    "                                     nn.Linear(in_features=300,\n",
    "                                               out_features=1000,\n",
    "                                               bias=True),\n",
    "                                     nn.ELU(alpha=1.0,\n",
    "                                            inplace=True),\n",
    "                                     nn.Dropout(p=0.5,\n",
    "                                                inplace=False),\n",
    "                                     nn.BatchNorm1d(num_features=1000,\n",
    "                                                    track_running_stats=False),\n",
    "                                     nn.Linear(in_features=1000,\n",
    "                                               out_features=4000,\n",
    "                                               bias=True),\n",
    "                                     nn.ELU(alpha=1.0,\n",
    "                                            inplace=True), \n",
    "                                     nn.BatchNorm1d(num_features=4000,\n",
    "                                                    track_running_stats=False),\n",
    "                                     nn.Linear(in_features=4000,\n",
    "                                               out_features=3 * 64 * 64,\n",
    "                                               bias=True),\n",
    "                                     nn.Sigmoid())\n",
    "\n",
    "    def forward(self,\n",
    "                x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vanilla_autoencoder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [2/50], train_bce_loss: -12.29821, val_bce_loss -11.90012\n",
      "epoch [3/50], train_bce_loss: -12.28460, val_bce_loss -11.91571\n",
      "epoch [4/50], train_bce_loss: -12.26610, val_bce_loss -11.75790\n",
      "epoch [5/50], train_bce_loss: -12.24715, val_bce_loss -11.98671\n",
      "epoch [6/50], train_bce_loss: -12.17251, val_bce_loss -11.63616\n",
      "epoch [7/50], train_bce_loss: -12.28583, val_bce_loss -11.58521\n",
      "epoch [8/50], train_bce_loss: -12.35167, val_bce_loss -11.95718\n",
      "epoch [9/50], train_bce_loss: -12.30204, val_bce_loss -11.76891\n",
      "epoch [10/50], train_bce_loss: -12.37299, val_bce_loss -11.87274\n",
      "epoch [11/50], train_bce_loss: -12.44932, val_bce_loss -11.83587\n",
      "epoch [12/50], train_bce_loss: -12.46879, val_bce_loss -11.64072\n",
      "epoch [13/50], train_bce_loss: -12.46968, val_bce_loss -11.58517\n",
      "epoch [14/50], train_bce_loss: -12.50143, val_bce_loss -11.28991\n",
      "epoch [15/50], train_bce_loss: -12.48203, val_bce_loss -11.48395\n",
      "epoch [16/50], train_bce_loss: -12.50637, val_bce_loss -11.30765\n",
      "epoch [17/50], train_bce_loss: -12.46775, val_bce_loss -11.38779\n",
      "epoch [18/50], train_bce_loss: -12.48509, val_bce_loss -11.22859\n",
      "epoch [19/50], train_bce_loss: -12.57873, val_bce_loss -11.18477\n",
      "epoch [20/50], train_bce_loss: -12.59074, val_bce_loss -11.25637\n",
      "epoch [21/50], train_bce_loss: -12.59074, val_bce_loss -11.37467\n",
      "epoch [22/50], train_bce_loss: -12.56368, val_bce_loss -11.34196\n",
      "epoch [23/50], train_bce_loss: -12.54269, val_bce_loss -11.23369\n",
      "epoch [24/50], train_bce_loss: -12.52998, val_bce_loss -10.97086\n",
      "epoch [25/50], train_bce_loss: -12.53826, val_bce_loss -11.19903\n",
      "epoch [26/50], train_bce_loss: -12.55486, val_bce_loss -10.85787\n",
      "epoch [27/50], train_bce_loss: -12.55961, val_bce_loss -11.04307\n",
      "epoch [28/50], train_bce_loss: -12.50725, val_bce_loss -10.97186\n",
      "epoch [29/50], train_bce_loss: -12.55362, val_bce_loss -10.84022\n",
      "epoch [30/50], train_bce_loss: -12.44766, val_bce_loss -11.02582\n",
      "epoch [31/50], train_bce_loss: -12.44718, val_bce_loss -10.97345\n",
      "epoch [32/50], train_bce_loss: -12.55251, val_bce_loss -11.00177\n",
      "epoch [33/50], train_bce_loss: -12.56066, val_bce_loss -10.79418\n",
      "epoch [34/50], train_bce_loss: -12.47977, val_bce_loss -10.84903\n",
      "epoch [35/50], train_bce_loss: -12.49965, val_bce_loss -10.94600\n",
      "epoch [36/50], train_bce_loss: -12.52467, val_bce_loss -10.66314\n",
      "epoch [37/50], train_bce_loss: -12.45791, val_bce_loss -10.65722\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-58e976246239>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m###################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# prep model for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# forward pass: compute predicted outputs by passing inputs to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/ml_dir/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/ml_dir/lib/python3.5/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg_fmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/ml_dir/lib/python3.5/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg_fmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/ml_dir/lib/python3.5/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                             weight_decay=0)\n",
    "LRStep = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                    patience=5,\n",
    "                                                    verbose=True,\n",
    "                                                    mode=\"min\")\n",
    "\n",
    "n_epochs = 50\n",
    "patience = 10\n",
    "\n",
    "model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "# to track the training loss as the model trains\n",
    "train_losses = []\n",
    "# to track the validation loss as the model trains\n",
    "valid_losses = []\n",
    "# to track the average training loss per epoch as the model trains\n",
    "avg_train_losses = []\n",
    "# to track the average validation loss per epoch as the model trains\n",
    "avg_valid_losses = [] \n",
    "\n",
    "# initialize the early_stopping object\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train() # prep model for training\n",
    "    for data, label in X_train_data:\n",
    "    \n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "              \n",
    "        data = data.view(data.size(0), -1)\n",
    "        data = Variable(data).to(device)\n",
    "        \n",
    "        data = data.type(torch.cuda.FloatTensor)\n",
    "        output = model.forward(data).to(device)\n",
    "        target = data\n",
    "                \n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        \n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # record training loss\n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    # print('epoch [{}/{}], train_loss:{:.4f}'.format(epoch + 1, n_epochs, mse_loss.data))\n",
    "\n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval() # prep model for evaluation\n",
    "    for data, label in X_cv_data:\n",
    "        \n",
    "        data = data.view(data.size(0), -1)\n",
    "        data = Variable(data).to(device)\n",
    "        \n",
    "        data = data.type(torch.cuda.FloatTensor)\n",
    "        output = model.forward(data).to(device)\n",
    "        target = data\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # record validation loss\n",
    "        valid_losses.append(loss.item())\n",
    "\n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = np.average(train_losses)\n",
    "    valid_loss = np.average(valid_losses)\n",
    "    avg_train_losses.append(train_loss)\n",
    "    avg_valid_losses.append(valid_loss)\n",
    "\n",
    "    \n",
    "    print_msg = \"epoch [{}/{}], train_bce_loss: {:.5f}, val_bce_loss {:.5f}\".format(epoch + 1, n_epochs, train_loss, valid_loss)\n",
    "\n",
    "    print(print_msg)\n",
    "\n",
    "    # clear lists to track next epoch\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    BCE_losses = []\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        plt.imsave('track.png', (Variable(output).data).cpu().numpy()[0].reshape(64, 64, 3))\n",
    "\n",
    "    # early_stopping needs the validation loss to check if it has decresed, \n",
    "    # and if it has, it will make a checkpoint of the current model\n",
    "    #early_stopping(valid_loss, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the last checkpoint with the best model\n",
    "model.load_state_dict(torch.load('checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
